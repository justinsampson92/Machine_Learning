<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Machine learning by justinsampson92</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Machine learning</h1>
        <h2>Spam Classifier by Geoff Kaufman, Chloe Lim, Dilip Ravindran, and Justin Sampson</h2>
        <a href="https://github.com/justinsampson92/Machine_Learning" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <p>This page represents the work of Geoff Kaufman, Chloe Lim, Dilip Ravindran, and Justin Sampson in the final project of Statistics 154: Machine Learning and Modern Statistical Prediction Fall 2014. Our code and report can be downloaded and viewed from this page. Feel free to contact any of us if you have questions about our work. Contact information is below.  </p>

<h1>
<a id="description-of-data" class="anchor" href="#description-of-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Description of data</h1>

<p>The data given is comprised of 4180 texts, one per line, each of which begins with one of the “spam” or “ham” labels, and the rest of the line representing the actual string of text. In total, there are 3615 ham texts and 565 spam texts (86.5% ham, 13.5% spam). The presence of the pound currency symbol and the 11-digit phone numbers as well as word spellings (“colour”) and website URLS (“.co.uk”) indicate that the texts are from the UK. Texts range in length from one to 80 words, including strings of numbers and punctuation, and appear to come from a variety of sources with a variety of intended recipients – some of the ham seem to be sent to friends, spouses, relatives, and others are sent from companies, coworkers, etc. Much of the language used in the texts appear to be shorthand (“u” instead of “you”, “cos” instead of “because”), in both spam and ham messages. </p>

<p>We conclude that the texts are everyday texts originating in the UK, and we treat them as if they are written in British English.</p>

<h1>
<a id="performance" class="anchor" href="#performance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Performance</h1>

<p>See project write-up for more details.</p>

<p>Accuracy = .986, PPV = 0.986, NPV = 0.987, Sensitivity = 0.912, Specificity = 0.997</p>

<p>These results show that the final classifier is fairly effective, identifying 91% of the spam messages while misclassifying only 4 of the 1300 ham messages. </p>

<p>The PPV is the important statistic here, and a higher PPV would be desired, since in this case 1.4% (~20) of the spam labels were actually ham.  In an actual application of spam targeting, this value would need to be much lower.</p>

<p>Testing these models informed us on the difference between power features and word features. We came to the conclusion that selected power features are much more powerful than raw word features. Automated spam prediction, informed purely by the words that appear in messages, is clearly less effective than a classifier that looks at some of the higher-level features of messages.</p>

<p>That being said, the raw word features seemed to have a beneficial effect on the PPV of the models. We would expect raw word features to have a better effect on predicting ham, since there will be a large variety of words in a given set of text messages, and raw word features can explain that variance.</p>

<p><img src="http://s8.postimg.org/skd9s80qd/roc.png" alt=""></p>

<h2>
<a id="contact-information" class="anchor" href="#contact-information" aria-hidden="true"><span class="octicon octicon-link"></span></a>Contact Information</h2>

<p><a href="mailto:gkaufman93@berkeley.edu">gkaufman93@berkeley.edu</a> 
<a href="mailto:chloelim@berkeley.edu">chloelim@berkeley.edu</a>
<a href="mailto:diliprr@berkeley.edu">diliprr@berkeley.edu</a> 
<a href="mailto:justinsampson1992@berkeley.edu">justinsampson1992@berkeley.edu</a> </p>
        </section>

        <aside id="sidebar">
          <a href="https://github.com/justinsampson92/Machine_Learning/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/justinsampson92/Machine_Learning/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/justinsampson92/Machine_Learning"></a> is maintained by <a href="https://github.com/justinsampson92">justinsampson92</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
