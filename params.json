{"name":"Machine learning","tagline":"Spam Classifier by Geoff Kaufman, Chloe Lim, Dilip Ravindran, and Justin Sampson","body":"This page represents the work of Geoff Kaufman, Chloe Lim, Dilip Ravindran, and Justin Sampson in the final project of Statistics 154: Machine Learning and Modern Statistical Prediction Fall 2014. Our code and report can be downloaded and viewed from this page. Feel free to contact any of us if you have questions about our work. Contact information is below.  \r\n\r\n# Description of data\r\n\r\nThe data given is comprised of 4180 texts, one per line, each of which begins with one of the “spam” or “ham” labels, and the rest of the line representing the actual string of text. In total, there are 3615 ham texts and 565 spam texts (86.5% ham, 13.5% spam). The presence of the pound currency symbol and the 11-digit phone numbers as well as word spellings (“colour”) and website URLS (“.co.uk”) indicate that the texts are from the UK. Texts range in length from one to 80 words, including strings of numbers and punctuation, and appear to come from a variety of sources with a variety of intended recipients – some of the ham seem to be sent to friends, spouses, relatives, and others are sent from companies, coworkers, etc. Much of the language used in the texts appear to be shorthand (“u” instead of “you”, “cos” instead of “because”), in both spam and ham messages. \r\n\r\nWe conclude that the texts are everyday texts originating in the UK, and we treat them as if they are written in British English.\r\n\r\n# Performance\r\n\r\nSee project write-up for more details.\r\n\r\nAccuracy = .986, PPV = 0.986, NPV = 0.987, Sensitivity = 0.912, Specificity = 0.997\r\n\r\nThese results show that the final classifier is fairly effective, identifying 91% of the spam messages while misclassifying only 4 of the 1300 ham messages. \r\n\r\nThe PPV is the important statistic here, and a higher PPV would be desired, since in this case 1.4% (~20) of the spam labels were actually ham.  In an actual application of spam targeting, this value would need to be much lower.\r\n\r\nTesting these models informed us on the difference between power features and word features. We came to the conclusion that selected power features are much more powerful than raw word features. Automated spam prediction, informed purely by the words that appear in messages, is clearly less effective than a classifier that looks at some of the higher-level features of messages.\r\n\r\nThat being said, the raw word features seemed to have a beneficial effect on the PPV of the models. We would expect raw word features to have a better effect on predicting ham, since there will be a large variety of words in a given set of text messages, and raw word features can explain that variance.\r\n\r\n\r\n![](http://s8.postimg.org/skd9s80qd/roc.png)\r\n\r\n## Contact Information\r\n\r\ngkaufman93@berkeley.edu \r\nchloelim@berkeley.edu\r\ndiliprr@berkeley.edu \r\njustinsampson1992@berkeley.edu \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}